## SLURM PROLOG ###############################################################
##    Job ID : 11078912
##  Job Name : dann_training
##  Nodelist : gpu2508
##      CPUs : 
##  Mem/Node : 196608 MB
## Directory : /oscar/data/rsingh47/wli115/universalgene/src/dann
##   Job Started : Mon Apr 21 02:40:44 PM EDT 2025
###############################################################################
/var/spool/slurmd/job11078912/slurm_script: line 18: ../pytorch.venv/bin/activate: No such file or directory
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name             | Type               | Params | Mode 
----------------------------------------------------------------
0 | encoder          | Encoder            | 2.1 M  | train
1 | domain_clf       | DomainClassifier   | 197 K  | train
2 | cell_clf         | CellClassifier     | 232 K  | train
3 | train_acc_cell   | MulticlassAccuracy | 0      | train
4 | train_acc_domain | MulticlassAccuracy | 0      | train
5 | val_acc_cell     | MulticlassAccuracy | 0      | train
6 | val_acc_domain   | MulticlassAccuracy | 0      | train
----------------------------------------------------------------
2.6 M     Trainable params
0         Non-trainable params
2.6 M     Total params
10.283    Total estimated model params size (MB)
17        Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
/oscar/data/rsingh47/wli115/scGrapHiCv2/pytorch.venv/lib64/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/oscar/data/rsingh47/wli115/scGrapHiCv2/pytorch.venv/lib64/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
`Trainer.fit` stopped: `max_epochs=100` reached.
