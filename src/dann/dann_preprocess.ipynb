{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "tm_droplet_data = sc.read(\n",
    "    r'./src/data/tabula_muris/TM_droplet.h5ad',\n",
    ")\n",
    "tm_facs_data = sc.read(\n",
    "    r'./src/data/tabula_muris/TM_facs.h5ad',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_droplet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tissue types in tm_droplet_data\n",
    "tm_droplet_data.obs[\"tissue\"].unique()\n",
    "# List all tissue types in tm_facs_data\n",
    "tm_facs_data.obs[\"tissue\"].unique()\n",
    "# List all cell types in tm_droplet_data\n",
    "tm_droplet_data.obs[\"cell_ontology_class\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only for cells with valid cell ontology class\n",
    "tm_droplet_data = tm_droplet_data[\n",
    "    (~tm_droplet_data.obs.cell_ontology_class.isna())\n",
    "].copy()\n",
    "tm_facs_data = tm_facs_data[\n",
    "    (~tm_facs_data.obs.cell_ontology_class.isna())\n",
    "].copy()\n",
    "\n",
    "# Add technology labels\n",
    "tm_droplet_data.obs[\"tech\"] = \"10x\"\n",
    "tm_facs_data.obs[\"tech\"] = \"SS2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_len = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/chenlingantelope/HarmonizationSCANVI/master/data/gene_len.txt\",\n",
    "    delimiter=\" \",\n",
    "    header=None,\n",
    "    index_col=0,\n",
    ")\n",
    "gene_len.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy import sparse\n",
    "\n",
    "# # First, get the gene length data and align it with the FACS data\n",
    "# gene_len = gene_len.reindex(tm_facs_data.var.index).dropna()\n",
    "# tm_facs_data = tm_facs_data[:, gene_len.index]\n",
    "\n",
    "# # Convert to sparse matrix if not already sparse\n",
    "# if not sparse.issparse(tm_facs_data.X):\n",
    "#     tm_facs_data.X = sparse.csr_matrix(tm_facs_data.X)\n",
    "\n",
    "# # Calculate the scaling factor once\n",
    "# scaling_factor = np.median(gene_len[1].values)\n",
    "\n",
    "# # Process in chunks to save memory\n",
    "# chunk_size = 10000  # Adjust based on your available memory\n",
    "# num_chunks = (tm_facs_data.shape[0] + chunk_size - 1) // chunk_size\n",
    "\n",
    "# for i in range(num_chunks):\n",
    "#     start_idx = i * chunk_size\n",
    "#     end_idx = min((i + 1) * chunk_size, tm_facs_data.shape[0])\n",
    "    \n",
    "#     # Get the chunk\n",
    "#     chunk = tm_facs_data.X[start_idx:end_idx]\n",
    "    \n",
    "#     # Convert to dense for the division operation\n",
    "#     chunk_dense = chunk.toarray()\n",
    "    \n",
    "#     # Apply the length normalization\n",
    "#     chunk_normalized = chunk_dense / gene_len[1].values * scaling_factor\n",
    "    \n",
    "#     # Round to integer\n",
    "#     chunk_rounded = np.rint(chunk_normalized)\n",
    "    \n",
    "#     # Convert back to sparse and update the original matrix\n",
    "#     tm_facs_data.X[start_idx:end_idx] = sparse.csr_matrix(chunk_rounded)\n",
    "    \n",
    "#     # Clear memory\n",
    "#     del chunk_dense, chunk_normalized, chunk_rounded\n",
    "\n",
    "# # Verify the operation\n",
    "# assert (tm_facs_data.var.index == gene_len.index).sum() == tm_facs_data.shape[1]\n",
    "import numpy as np\n",
    "\n",
    "gene_len = gene_len.reindex(tm_facs_data.var.index).dropna()\n",
    "tm_facs_data = tm_facs_data[:, gene_len.index]\n",
    "assert (tm_facs_data.var.index == gene_len.index).sum() == tm_facs_data.shape[1]\n",
    "tm_facs_data.X = tm_facs_data.X / gene_len[1].values * np.median(gene_len[1].values)\n",
    "# round to integer\n",
    "tm_facs_data.X = np.rint(tm_facs_data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "tm_adata = tm_droplet_data.concatenate(tm_facs_data)\n",
    "tm_adata.layers[\"counts\"] = tm_adata.X.copy()\n",
    "sc.pp.normalize_total(tm_adata, target_sum=1e4)\n",
    "sc.pp.log1p(tm_adata)\n",
    "tm_adata.raw = tm_adata  # keep full dimension safe\n",
    "sc.pp.highly_variable_genes(\n",
    "    tm_adata,\n",
    "    flavor=\"seurat_v3\",\n",
    "    n_top_genes=2000,\n",
    "    layer=\"counts\",\n",
    "    batch_key=\"tech\",\n",
    "    subset=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Create the directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(r'./src/data/dann', exist_ok=True)\n",
    "\n",
    "with open(r'./src/data/dann/all_cell_data.pkl', 'wb') as f:\n",
    "    pickle.dump(tm_adata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640ad77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
