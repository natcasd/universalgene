{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n"
     ]
    }
   ],
   "source": [
    "# Create necessary directories\n",
    "os.makedirs('./models/dann/', exist_ok=True)\n",
    "\n",
    "# 1. Data Loading and Preparation\n",
    "print(\"Loading and preparing data...\")\n",
    "with open('./src/data/dann/all_cell_data.pkl', 'rb') as f:\n",
    "    adata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Standardize the data\u001b[39;00m\n\u001b[32m      6\u001b[39m scaler = StandardScaler()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[32m     10\u001b[39m X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/oscar/scratch/jfinberg/universalgene/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/oscar/scratch/jfinberg/universalgene/.venv/lib/python3.11/site-packages/sklearn/base.py:1098\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m   1083\u001b[39m         warnings.warn(\n\u001b[32m   1084\u001b[39m             (\n\u001b[32m   1085\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1093\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   1094\u001b[39m         )\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1098\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1100\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/oscar/scratch/jfinberg/universalgene/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:878\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/oscar/scratch/jfinberg/universalgene/.venv/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/oscar/scratch/jfinberg/universalgene/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:942\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m    941\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    943\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot center sparse matrices: pass `with_mean=False` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    944\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minstead. See docstring for motivation and alternatives.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    945\u001b[39m         )\n\u001b[32m    946\u001b[39m     sparse_constructor = (\n\u001b[32m    947\u001b[39m         sparse.csr_matrix \u001b[38;5;28;01mif\u001b[39;00m X.format == \u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m sparse.csc_matrix\n\u001b[32m    948\u001b[39m     )\n\u001b[32m    950\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_std:\n\u001b[32m    951\u001b[39m         \u001b[38;5;66;03m# First pass\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives."
     ]
    }
   ],
   "source": [
    "# Extract gene expression data and domain labels\n",
    "X = adata.X  # Gene expression matrix\n",
    "domain_labels = adata.obs['tech'].astype('category').cat.codes  # Domain labels (10x vs SS2)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "domain_tensor = torch.tensor(domain_labels.values, dtype=torch.long)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, domain_train, domain_val = train_test_split(\n",
    "    X_tensor, domain_tensor, test_size=0.2, random_state=42, stratify=domain_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Architecture\n",
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return F.relu(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)  # Binary classification: 10x vs SS2\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, alpha=1.0):\n",
    "        x = GradientReversalLayer.apply(x, alpha)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return F.log_softmax(self.fc2(x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset and Training Setup\n",
    "class SingleCellDataset(Dataset):\n",
    "    def __init__(self, X, domain_labels):\n",
    "        self.X = X\n",
    "        self.domain_labels = domain_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.domain_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = X_tensor.shape[1]\n",
    "hidden_dim = 512\n",
    "lambda_domain = 1.0\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = SingleCellDataset(X_train, domain_train)\n",
    "val_dataset = SingleCellDataset(X_val, domain_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(input_dim, hidden_dim)\n",
    "domain_classifier = DomainClassifier(128)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(domain_classifier.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training Loop\n",
    "def train_epoch(encoder, domain_classifier, train_loader, optimizer, lambda_domain):\n",
    "    encoder.train()\n",
    "    domain_classifier.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_domain in train_loader:\n",
    "        # Forward pass\n",
    "        encoded = encoder(batch_X)\n",
    "        domain_preds = domain_classifier(encoded)\n",
    "        \n",
    "        # Calculate losses\n",
    "        domain_loss = F.nll_loss(domain_preds, batch_domain)\n",
    "        contrastive_loss = torch.mean((encoded - encoded) ** 2)  # Example contrastive loss\n",
    "        \n",
    "        # Total loss\n",
    "        loss = contrastive_loss - lambda_domain * domain_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(encoder, domain_classifier, val_loader, lambda_domain):\n",
    "    encoder.eval()\n",
    "    domain_classifier.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_domain in val_loader:\n",
    "            encoded = encoder(batch_X)\n",
    "            domain_preds = domain_classifier(encoded)\n",
    "            \n",
    "            domain_loss = F.nll_loss(domain_preds, batch_domain)\n",
    "            contrastive_loss = torch.mean((encoded - encoded) ** 2)\n",
    "            \n",
    "            loss = contrastive_loss - lambda_domain * domain_loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(encoder, domain_classifier, train_loader, optimizer, lambda_domain)\n",
    "    val_loss = validate(encoder, domain_classifier, val_loader, lambda_domain)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# 5. Visualization and Analysis\n",
    "print(\"Generating visualizations...\")\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.savefig('./models/training_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# Get embeddings for visualization\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = encoder(X_tensor).numpy()\n",
    "\n",
    "# Plot embeddings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=embeddings[:, 0], y=embeddings[:, 1], hue=domain_labels)\n",
    "plt.title('DANN Embeddings by Domain')\n",
    "plt.savefig('./models/embeddings.png')\n",
    "plt.close()\n",
    "\n",
    "# 6. Save Model\n",
    "print(\"Saving models...\")\n",
    "torch.save(encoder.state_dict(), './models/dann_encoder.pth')\n",
    "torch.save(domain_classifier.state_dict(), './models/dann_domain_classifier.pth')\n",
    "\n",
    "print(\"Training complete!\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
