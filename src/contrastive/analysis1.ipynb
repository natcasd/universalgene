{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_contrastive import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/tabula_muris/preprocessed_reduced/tm_adata_test.pkl\"\n",
    "\n",
    "X, domains, cells, adata = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contrastive_model import ContrastiveModel\n",
    "\n",
    "c_transformer_path = \"runs/attention_05-10-20:43/final.ckpt\"\n",
    "\n",
    "c_transformer = ContrastiveModel.load_from_checkpoint(c_transformer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContrastiveModel(\n",
       "  (encoder): AttentionEncoder(\n",
       "    (expr_proj): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projection_head): ProjectionHead(\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda_device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set tensors to the appropriate device\n",
    "x = torch.tensor(X, dtype=torch.float32).to(cuda_device)\n",
    "\n",
    "c_transformer.to(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|███████████████████████████████████████████████████████| 1931/1931 [01:26<00:00, 22.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32  # Define a suitable batch size\n",
    "ct_embeddings_list = []\n",
    "\n",
    "c_transformer.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, x.size(0), batch_size), desc=\"Processing Batches\"):\n",
    "        x_r_batch = x[i:i + batch_size]\n",
    "        \n",
    "        ct_embeddings_batch = c_transformer.encoder(x_r_batch)\n",
    "        \n",
    "        ct_embeddings_list.append(ct_embeddings_batch)\n",
    "\n",
    "ct_embeddings = torch.cat(ct_embeddings_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2571819/1969382214.py:1: ImplicitModificationWarning: Setting element `.obsm['ct_embeddings']` of view, initializing view as actual.\n",
      "  adata.obsm[\"ct_embeddings\"] = ct_embeddings.cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "adata.obsm[\"ct_embeddings\"] = ct_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/anaconda-2023.09-0-7nso27ys7navjquejqdxqylhg7kuyvxo/lib/python3.11/site-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "sc.pp.neighbors(adata, use_rep=\"ct_embeddings\")  # Compute neighbors using latent space\n",
    "sc.tl.umap(adata)\n",
    "fig1 = sc.pl.umap(adata, color='cell_ontology_class', legend_loc=None, title='', frameon=False, size=50, return_fig=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Extract the necessary data\n",
    "cell_ontology_classes = adata.obs['cell_ontology_class']\n",
    "tech_types = adata.obs['tech']\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame({'cell_ontology_class': cell_ontology_classes, 'tech': tech_types})\n",
    "\n",
    "# Group by 'cell_ontology_class' and 'tech' to count occurrences\n",
    "grouped = df.groupby(['cell_ontology_class', 'tech']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot a bar graph for each category in 'cell_ontology_class' with bars side by side\n",
    "grouped.plot(kind='bar', stacked=False, figsize=(12, 8))\n",
    "\n",
    "plt.title('Number of Examples with Each \"Tech\" Type per Cell Ontology Class')\n",
    "plt.xlabel('Cell Ontology Class')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.legend(title='Tech Type')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('analysis1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ndepiero/.local/lib/python3.11/site-packages/scanpy/plotting/_utils.py:487: ImplicitModificationWarning: Trying to modify attribute `._uns` of view, initializing view as actual.\n",
      "  adata.uns[value_to_plot + \"_colors\"] = colors_list\n"
     ]
    }
   ],
   "source": [
    "# Filter the data to include only \"basal cell of epidermis\"\n",
    "basal_cells = adata[adata.obs['cell_ontology_class'] == 'skeletal muscle satellite cell']\n",
    "\n",
    "# Plot UMAP, coloring by 'tech'\n",
    "fig2 = sc.pl.umap(basal_cells, color='tech', legend_loc='right margin', title='UMAP of Basal Cells of Epidermis by Tech', frameon=False, size=50, return_fig=True, show=False)\n",
    "fig2.savefig('analy2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 most occurring cell_ontology_classes\n",
    "top_10_classes = adata.obs['cell_ontology_class'].value_counts().nlargest(10).index\n",
    "\n",
    "# Filter the adata to include only the top 10 classes\n",
    "filtered_adata = adata[adata.obs['cell_ontology_class'].isin(top_10_classes)]\n",
    "\n",
    "# Plot UMAP for the filtered data\n",
    "fig1_top10 = sc.pl.umap(filtered_adata, color='cell_ontology_class', legend_loc=None, title='Top 10 Cell Ontology Classes', frameon=False, size=50, return_fig=True, show=True)\n",
    "fig1_top10.savefig('analy3.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python env for universalgene project",
   "language": "python",
   "name": "universalgene"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
