## SLURM PROLOG ###############################################################
##    Job ID : 11237560
##  Job Name : dann_training
##  Nodelist : gpu2503
##      CPUs : 
##  Mem/Node : 196608 MB
## Directory : /oscar/scratch/edalal/universalgene
##   Job Started : Fri May  2 09:52:46 PM EDT 2025
###############################################################################
You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A5500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type               | Params | Mode 
----------------------------------------------------------------
0 | encoder          | TransformerEncoder | 13.7 M | train
1 | domain_clf       | DomainClassifier   | 197 K  | train
2 | cell_clf         | CellClassifier     | 237 K  | train
3 | train_acc_cell   | MulticlassAccuracy | 0      | train
4 | train_acc_domain | MulticlassAccuracy | 0      | train
5 | val_acc_cell     | MulticlassAccuracy | 0      | train
6 | val_acc_domain   | MulticlassAccuracy | 0      | train
----------------------------------------------------------------
14.1 M    Trainable params
0         Non-trainable params
14.1 M    Total params
56.539    Total estimated model params size (MB)
57        Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
`Trainer.fit` stopped: `max_epochs=100` reached.
